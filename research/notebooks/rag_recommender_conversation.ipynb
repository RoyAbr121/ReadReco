{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-09T10:51:24.257283Z",
     "start_time": "2024-10-09T10:51:24.252491Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.vectorstores import Chroma, Milvus\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T10:51:25.675030Z",
     "start_time": "2024-10-09T10:51:24.278287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Load the existing vector store with the embedding function\n",
    "db = Milvus(embedding_function=embeddings, collection_name=\"novels\",\n",
    "            connection_args={\"host\": \"127.0.0.1\", \"port\": \"19530\"})\n",
    "\n",
    "# Create a retriever for querying the vector store\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ],
   "id": "e36571be78935ba8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T10:51:25.685901Z",
     "start_time": "2024-10-09T10:51:25.682131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Contextualize question prompt\n",
    "contextualize_q_system_prompt = \"Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
    "\n",
    "# Create a prompt template for contextualizing questions\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([(\"system\", contextualize_q_system_prompt), \n",
    "                                                           MessagesPlaceholder(\"chat_history\"), \n",
    "                                                           (\"human\", \"{input}\"),])\n",
    "\n",
    "# Create a history-aware retriever\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "\n",
    "# Answer question prompt\n",
    "qa_system_prompt = \"You are a helpful book recommender. Use the following pieces of retrieved context for books to recommend titles based on the request. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\"\n",
    "\n",
    "# Create a prompt template for answering questions\n",
    "qa_prompt = ChatPromptTemplate.from_messages([(\"system\", qa_system_prompt), \n",
    "                                              MessagesPlaceholder(\"chat_history\"), \n",
    "                                              (\"human\", \"{input}\")])\n",
    "\n",
    "# Create a chain to combine documents for question answering\n",
    "# `create_stuff_documents_chain` - feeds all retrieved context into the LLM\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Create a retrieval chain that combines the history-aware retriever and the question answering chain\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n"
   ],
   "id": "6bcf4da9174af740",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T10:51:54.288108Z",
     "start_time": "2024-10-09T10:51:25.693022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to simulate a continual chat\n",
    "def continual_chat():\n",
    "    print(\"Start chatting with the AI! Type 'exit' to end the conversation.\")\n",
    "    chat_history = []  # Collect chat history here (a sequence of messages)\n",
    "\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        # Process the user's query through the retrieval chain\n",
    "        result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
    "        \n",
    "        # Display the AI's response\n",
    "        print(f\"AI: {result['answer']}\")\n",
    "        \n",
    "        # Update the chat history\n",
    "        chat_history.append(HumanMessage(content=query))\n",
    "        chat_history.append(SystemMessage(content=result[\"answer\"]))\n",
    "\n",
    "# Main function to start the continual chat\n",
    "if __name__ == \"__main__\":\n",
    "    continual_chat()"
   ],
   "id": "9bc0393ba6515568",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the AI! Type 'exit' to end the conversation.\n",
      "AI: I recommend \"The Adventures of Huckleberry Finn\" by Mark Twain. Set in the Mississippi Valley, it captures the spirit of America forty to fifty years before its publication, exploring themes of freedom, friendship, and social issues. It's a classic American novel that offers both adventure and insight into the country's history.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 24\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;66;03m# Main function to start the continual chat\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m---> 24\u001B[0m     \u001B[43mcontinual_chat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[10], line 7\u001B[0m, in \u001B[0;36mcontinual_chat\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m chat_history \u001B[38;5;241m=\u001B[39m []  \u001B[38;5;66;03m# Collect chat history here (a sequence of messages)\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m----> 7\u001B[0m     query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mYou: \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m query\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexit\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m     10\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py:1282\u001B[0m, in \u001B[0;36mKernel.raw_input\u001B[1;34m(self, prompt)\u001B[0m\n\u001B[0;32m   1280\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1281\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[1;32m-> 1282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1283\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1284\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1285\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1286\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1287\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py:1325\u001B[0m, in \u001B[0;36mKernel._input_request\u001B[1;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[0;32m   1322\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[0;32m   1323\u001B[0m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[0;32m   1324\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInterrupted by user\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1325\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1326\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1327\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Message:\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
